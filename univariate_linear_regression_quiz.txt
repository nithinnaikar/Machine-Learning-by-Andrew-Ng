1. m = 4
2. theta_0 = -569.6, theta_1 = -530.9
3. h_theta(6) = 1
4. 1) if theta_0 and theta_1 are initialized at the global minimum, then one iteration will not change their values 2) if the first few iterations of gradient descent cause
f(theta_0, theta_1) to increase rather than decrease, then the most likely cause is we have set the learning rate alpha to too large a value.
5. For these values of theta_0 and theta_1 that satisfy J(theta_0, theta_1) = 0, we have that h_theta(x_i) = y_i for every training example (x_i, y_i)

Grade Received: 100%
